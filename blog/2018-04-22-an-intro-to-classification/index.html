<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    
        <meta name="twitter:card" content="summary"/>
    



<meta name="twitter:title" content="An Intro to Classification"/>
<meta name="twitter:description" content=""/>
<meta name="twitter:site" content="@"/>



  	<meta property="og:title" content="An Intro to Classification &middot; STOR-i Blog" />
  	<meta property="og:site_name" content="STOR-i Blog" />
  	<meta property="og:url" content="/blog/2018-04-22-an-intro-to-classification/" />

    
        
            <meta property="og:image" content="/images/cover.jpg"/>
        
    

    
    <meta property="og:description" content="" />
  	<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2018-04-22T00:00:00Z" />

    
    <meta property="article:tag" content="Classification" />
    
    

    <title>An Intro to Classification &middot; STOR-i Blog</title>

    
    <meta name="description" content="So, this is a follow on from the Supervised or not blog post where I looked at how to decide if a problem is supervised or unsupervised and looked at a simple e" />
    

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/images/favicon.ico">
	  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="/css/nav.css" />

    

    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
      
          <link href="/index.xml" rel="alternate" type="application/rss+xml" title="STOR-i Blog" />
      
      
    
    <meta name="generator" content="Hugo 0.39" />

    <link rel="canonical" href="/blog/2018-04-22-an-intro-to-classification/" />

    
      
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": ,
        "logo": /images/logo.png
    },
    "author": {
        "@type": "Person",
        "name": ,
        
        "image": {
            "@type": "ImageObject",
            "url": /images/logo.png,
            "width": 250,
            "height": 250
        }, 
        
        "url": http://lancaster.ac.uk/~omalley3,
        "sameAs": [
            
            
             
             
             
             
             
            
        ],
        "description": Statistics &amp; Operations Student
        
    },
    "headline": An Intro to Classification,
    "name": An Intro to Classification,
    "wordCount": 1273,
    "timeRequired": "PT6M",
    "inLanguage": {
      "@type": "Language",
      "alternateName": en
    },
    "url": /blog/2018-04-22-an-intro-to-classification/,
    "datePublished": 2018-04-22T00:00Z,
    "dateModified": 2018-04-22T00:00Z,
    
    "keywords": Classification,
    "description": ,
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": /blog/2018-04-22-an-intro-to-classification/
    }
}
    </script>
    


    

    
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-111132558', 'auto');
      ga('send', 'pageview');

    </script>
    

    
</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="http://lancaster.ac.uk/~omalley3">My Website</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/">Blog Entries</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="http://www.lancaster.ac.uk/stor-i/">STOR-i Website</a>
            </li>
        
        
    </ul>

    
    <a class="subscribe-button icon-feed" href="/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">



<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">


  
      <a class="blog-logo" href="/"><img src="/images/logo.png" alt="Home" /></a>
  
  
      <a class="menu-button" href="#"><span class="burger">&#9776;</span><span class="word">Menu</span></a>
  
  </nav>
</header>



<main class="content" role="main">




  <article class="post post">

    <header class="post-header">
        <h1 class="post-title">An Intro to Classification</h1>
        <small></small>

        <section class="post-meta">
        
          <time class="post-date" datetime="2018-04-22T00:00:00Z">
            Apr 22, 2018
          </time>
        
         
          <span class="post-tag small"><a href="//tags/classification/">#Classification</a></span>
         
        </section>
    </header>

    <section class="post-content">
      <p>So, this is a follow on from the <a href="https://mikeoma.github.io/blog/2018-03-20-supervised-or-not/">Supervised or not blog post</a> where I looked at how to decide if a problem is supervised or unsupervised and looked at a simple example on the iris dataset. Similar to that post, here I’ll look at classification again, but we’ll go more in-depth into some issues with classification.</p>
<div id="linear-discriminant-analysis" class="section level2">
<h2>Linear Discriminant Analysis</h2>
<p>In the previous post we’ve used K-nn, here we’ll use Linear discriminant analysis (LDA) which is slightly more complicated. It makes the assumption that the points in each class <span class="math inline">\(k\)</span> follow a normal distribution with mean <span class="math inline">\(\mu_k\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> where the variance is the same across all the groups.</p>
<p>So this time we have the same setup <span class="math inline">\(n\)</span> data-points <span class="math inline">\(x_i\)</span> and each data-point has a response <span class="math inline">\(y_i\)</span>. The goal is to fit a model which gets P(Y=k| X=x), so the probability that the class is k given the data-point <span class="math inline">\(x\)</span>. We then derive this discriminant function <span class="math inline">\(\delta_j(x_i)\)</span> which basically forms a rule that the j with the highest value of <span class="math inline">\(\delta_j\)</span> will be the most likely class, and therefore our prediction.</p>
<p>The beauty of this method is that is gives us a probability that our prediction is correct using the formula.</p>
<div id="why-the-name" class="section level4">
<h4>Why the name?</h4>
<p>So the assumption of equal variance and different means actually results in a <em>linear boundary</em> between each class. That is if you draw a line in the feature space <span class="math inline">\(x\)</span> where the prediction rule swaps between assigning the point to class <span class="math inline">\(k_1\)</span> or <span class="math inline">\(k_2\)</span> that line will be a linear one; hence linear discriminant analysis. A similar algorithm is called Quadratic Discriminant Analysis (QDA), which is very similar however we remove the assumption that the covariance matrix is equal across all groups, this results in that separating line to be a quadratic formula.</p>
</div>
</div>
<div id="example-default-dataset" class="section level2">
<h2>Example: Default Dataset</h2>
<p>So to show why such a model is more useful and illustrate some issues and factors to consider in classification, I’ll run through an example of LDA on the Default dataset.</p>
<p>Each row is a person. The dataset has response data:</p>
<ul>
<li><span class="math inline">\(y\)</span> called default which says if a person defaults on their loan in future or not</li>
<li><span class="math inline">\(x_{,1}\)</span> That persons current balance.</li>
<li><span class="math inline">\(x_{,2}\)</span> That persons’ income.</li>
<li><span class="math inline">\(x_{,3}\)</span> If the person is a student, here I’ll drop all such entries as we only want to use continuous variables.</li>
</ul>
<pre class="r"><code>library(ISLR)
library(ggplot2)
head(Default)</code></pre>
<pre><code>##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559</code></pre>
<pre class="r"><code>#We&#39;ll ignore student variable &amp; entries
dataset &lt;- Default[c(1,3,4)] 
dataset &lt;- dataset[-which(
            Default$student==&quot;Yes&quot;
            ),]
n &lt;- nrow(dataset)

ggplot(dataset, aes(x=balance,y=income, color=default))+geom_point()</code></pre>
<p><img src="/post/2018-04-22-an-intro-to-classification_files/figure-html/Load%20data-1.png" width="672" /> So there’s our data after removing all the student entries. Now, I’ll fit lda using the mass package.</p>
<pre class="r"><code>library(MASS)

fit &lt;- lda(default ~ balance+income, data=dataset)
fit</code></pre>
<pre><code>## Call:
## lda(default ~ balance + income, data = dataset)
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.97080499 0.02919501 
## 
## Group means:
##       balance   income
## No   744.5044 39993.52
## Yes 1678.4295 40625.05
## 
## Coefficients of linear discriminants:
##                  LD1
## balance 2.258357e-03
## income  2.725668e-06</code></pre>
<p>The prior probabilities and coefficients are to do with the fitting of the discriminant function, we’ll ignore those here but if you’re interested in the underlying maths have a look at <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Chapter 5 in Elements of Statistical Learning</a></p>
<p>The details to note is that the people will higher balances are the ones who are predicted to default. There’s also the question how good the model is, to do this we can look at various diagnostics.</p>
<div id="training-error" class="section level4">
<h4>Training error</h4>
<p>The first diagnostic we’ll look at is something called the training error, it’s how often the model is correct if you we’re to predict the same <span class="math inline">\(y_i\)</span>’s using the <span class="math inline">\(x_i\)</span> corresponding to it and the model we fit.</p>
<pre class="r"><code>#obtain predictions 
train.preds &lt;- predict(fit)
#find % which are correct
train.err &lt;- mean(train.preds$class==dataset$default)
train.err</code></pre>
<pre><code>## [1] 0.9757653</code></pre>
<p>This is ridiculously high for a model, but this percentage is often misleading, for example let’s try another model as such, we just say nobody defaults.</p>
<pre class="r"><code>#Initialize
dummypred &lt;- dataset$default
#Change all to No, first entry is no so use that
dummypred &lt;- dummypred[1]
mean(dummypred==dataset$default)</code></pre>
<pre><code>## [1] 0.970805</code></pre>
<p>Yeah so initially we got just slightly higher than an extremely stupid model, sounds bad right? So we’ll look at why that 0.5% increase is important.</p>
</div>
<div id="cross-tabulation" class="section level4">
<h4>Cross Tabulation</h4>
<p>A better way to analyse classification results is known as a cross tabulation. You simply look at a table like so:</p>
<pre class="r"><code>## Num of training errors 
n-sum(train.preds$class==dataset$default)</code></pre>
<pre><code>## [1] 171</code></pre>
<pre class="r"><code>#Make crosstab
table(train.preds$class,dataset$default)</code></pre>
<pre><code>##      
##         No  Yes
##   No  6838  159
##   Yes   12   47</code></pre>
<p>This is far more interesting than the number, it shows us the breakdown of the training error so the horizontal axis is the predictions and vertical is the true labels. 12 of our 171 errors occurred by saying No to people who actually do default, the other 159 were saying yes about people who actually don’t default. Now we’ll look at why LDA can be more useful.</p>
</div>
<div id="a-benefit-of-lda" class="section level4">
<h4>A benefit of LDA</h4>
<p>LDA is a statistical model, if the assumptions turn out to be true (they almost never are) we get a pretty good estimate of <span class="math inline">\(P(Y=k|X=x)\)</span>. Even when they’re not true the estimate is pretty close, so here we’ll look at the probabilities for the miss classified observations.</p>
<pre class="r"><code>#index of misclassifications
miscl &lt;-train.preds$class!=dataset$default
misclYes &lt;- miscl &amp; dataset$default==&quot;Yes&quot;
misclNo  &lt;- miscl &amp; dataset$default==&quot;No&quot;
colarray &lt;-rep(&quot;Correct,Yes&quot;,n)
colarray[dataset$default==&quot;No&quot;] &lt;- &quot;Correct, No&quot;
colarray[misclYes]&lt;- &quot;Wrong, Yes&quot;
colarray[misclNo]&lt;- &quot;Wrong, No&quot;
colarray &lt;- as.factor(colarray)
ggplot()+geom_boxplot(aes(y=as.numeric(train.preds$posterior[,1]),x=colarray))+ylab(&quot;Probability of No&quot;)</code></pre>
<p><img src="/post/2018-04-22-an-intro-to-classification_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>There’s a few things to note from this:</p>
<ul>
<li>Correct, No: this is quite good as most of the probabilities are up around 1 meaning we’re usually fairly certain that they won’t default with few cases down lower past 0.9.</li>
<li>Correct, Yes: most of these probabilities lie in the [0.7,0.6] range of predicting they will default. Meaning we can rarely strongly state that the person will default.</li>
<li>Wrong, No: when the prediction was wrong but the true answer was no. This is extremely good as nearly all points are in the [0.35,0.45] range of being No which means our model doesn’t do too badly here.</li>
<li>Wrong, Yes: Most of these have quite a low probability of being Yes which is disappointing as it shows our model does poorly.</li>
</ul>
<p>If you went for a rule of something like “if the model is more than 10% uncertain, carry out further checks” may be a good decision. However even the observations which we predicted correctly (Correct, yes, no) have quite a large amount of uncertainty for some observations so this may result in unneccessary work which may be avoided by using a different model or dataset.</p>
<p>An analysis like this would be impossible with k-nearest neighbours therefore motivating our choice. Statistical models usually give other insights which can then be useful for assessing risk or making decisions.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>We looked at some basics of classification and how to fit a statistical classifier. Then looked at the training error rate and how it can be misleading by comparing to a dummy classifier. The cross tabulation can often be a good tool to check where the model does poorly and can be used for any classifier.</p>
<p>The final analysis on the probabilities is quite a desirable property of a model and is often impossible to do with other types of models. It gives you uncertainties about your predictions.</p>
<p>In the future, I’ll do a brief blog post on model validation and comparison which will move onto the topic of picking which model is best.</p>
</div>

    </section>


  <footer class="post-footer">


    








<figure class="author-image">
    <a class="img" href="/" style="background-image: url(/images/logo.png)"><span class="hidden">Mike's Picture</span></a>
</figure>


<section class="author">
  <h4><a href="/">Mike</a></h4>
  
  <p>Statistics &amp; Operations Student</p>
  
  <div class="author-meta">
    <span class="author-location icon-location">STOR-i, Lancaster University</span>
    <span class="author-link icon-link"><a href="http://lancaster.ac.uk/~omalley3">http://lancaster.ac.uk/~omalley3</a></span>
  </div>
</section>




    
<section class="share">
  <h4>Share this post</h4>
  <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=An%20Intro%20to%20Classification&nbsp;-&nbsp;STOR-i%20Blog&amp;url=%2fblog%2f2018-04-22-an-intro-to-classification%2f"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      <span class="hidden">Twitter</span>
  </a>
  <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=%2fblog%2f2018-04-22-an-intro-to-classification%2f"
      onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      <span class="hidden">Facebook</span>
  </a>
  <a class="icon-pinterest" style="font-size: 1.4em" href="http://pinterest.com/pin/create/button/?url=%2fblog%2f2018-04-22-an-intro-to-classification%2f&amp;description=An%20Intro%20to%20Classification"
      onclick="window.open(this.href, 'pinterest-share','width=580,height=296');return false;">
      <span class="hidden">Pinterest</span>
  </a>
  <a class="icon-google-plus" style="font-size: 1.4em" href="https://plus.google.com/share?url=%2fblog%2f2018-04-22-an-intro-to-classification%2f"
     onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
      <span class="hidden">Google+</span>
  </a>
</section>



    

<div id="disqus_thread"></div>
<script>




var disqus_config = function () {
this.page.url = "\/blog\/2018-04-22-an-intro-to-classification\/";  
this.page.identifier = "\/blog\/2018-04-22-an-intro-to-classification\/"; 
};

(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://mikeoma-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>








  </footer>
</article>

</main>


  <aside class="read-next">
  
      <a class="read-next-story" style="no-cover" href="/blog/2018-04-23-model-selection/">
          <section class="post">
              <h2>Model Selection</h2>
              
          </section>
      </a>
  
  
      <a class="read-next-story prev" style="no-cover" href="/blog/2018-04-22-model-selection/">
          <section class="post">
              <h2>Model Selection</h2>
          </section>
      </a>
  
</aside>



    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">STOR-i Blog</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="http://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/vjeantet/hugo-theme-casper">Casper</a> theme</section>
        
    </footer>
    </div>

    <script type="text/javascript" src="/js/jquery.js"></script>
    <script type="text/javascript" src="/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/js/index.js"></script>
    
</body>
</html>

