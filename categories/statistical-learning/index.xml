<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Learning on STOR-i Blog</title>
    <link>https://mikeoma.github.io/categories/statistical-learning/</link>
    <description>Recent content in Statistical Learning on STOR-i Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mikeoma.github.io/categories/statistical-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Model Selection</title>
      <link>https://mikeoma.github.io/blog/2018-04-23-model-selection/</link>
      <pubDate>Mon, 23 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mikeoma.github.io/blog/2018-04-23-model-selection/</guid>
      <description>I’ve done a couple blogposts in the past on Statistical learning, see here if you havn’t read them yet. In this blog post I’ll explain the most popular way to compare models and decide which one is best. It’s known as the test-train split. This is really only useful for supervised problems.
The test set approach So the test-set approach is quite intuitive when you hear about it. You have your \(n\) data-points you observed each of which has explanatory \(x_i\) and response \(y_i\) and our end goal is to predict the \(y_i\).</description>
    </item>
    
    <item>
      <title>An Intro to Classification</title>
      <link>https://mikeoma.github.io/blog/2018-04-22-an-intro-to-classification/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mikeoma.github.io/blog/2018-04-22-an-intro-to-classification/</guid>
      <description>So, this is a follow on from the Supervised or not blog post where I looked at how to decide if a problem is supervised or unsupervised and looked at a simple example on the iris dataset. Similar to that post, here I’ll look at classification again, but we’ll go more in-depth into some issues with classification.
Linear Discriminant Analysis In the previous post we’ve used K-nn, here we’ll use Linear discriminant analysis (LDA) which is slightly more complicated.</description>
    </item>
    
    <item>
      <title>AI ain&#39;t here yet!</title>
      <link>https://mikeoma.github.io/blog/2018-04-19-ai-aint-here-yet/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mikeoma.github.io/blog/2018-04-19-ai-aint-here-yet/</guid>
      <description>This blog post is about a talk given by Prof. Michael Jordan at SysML conference on the 15th of February.. He&amp;rsquo;s a professor of Statistics in the Department of Electrical Engineering and Computer Science and the Department of Statistics at the University of California, Berkeley. He&amp;rsquo;s extremely well-known, and has over 130,000 citations on google scholar. This is very much a follow on post from my previous blog post The Two Cultures of Data Analysis.</description>
    </item>
    
    <item>
      <title>The Two Cultures of Data Analysis</title>
      <link>https://mikeoma.github.io/blog/2018-03-26-the-two-cultures-of-data-analysis/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mikeoma.github.io/blog/2018-03-26-the-two-cultures-of-data-analysis/</guid>
      <description>Much of the reading I do tends to end up leading me to many papers which seem to be carried out in the machine learning field rather than statistics. I always ask myself what’s the difference. There’s so much blurring between the two areas like topic modelling which is based off of Bayesian statistics but still is worked on primarily by people in the machine learning community. Then, there’re areas which fall more in the statistics field like Expectation-Maximization; and the computer science field like neural networks.</description>
    </item>
    
    <item>
      <title>Supervised or not?</title>
      <link>https://mikeoma.github.io/blog/2018-03-20-supervised-or-not/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mikeoma.github.io/blog/2018-03-20-supervised-or-not/</guid>
      <description>MACHINE LEARNING!!! So, if you’ve not heard of machine learning yet, you probably haven’t been watching any TV the last decade. Problem is machine learning is absolutely massive field.
This is the intro to a series of blog posts I plan on doing on various areas in machine learning, more specifically statistically backed methods therefore I call it statistical learning. In this blog post I aim to break down the two main areas that are generally focused on.</description>
    </item>
    
  </channel>
</rss>