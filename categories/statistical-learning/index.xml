<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Learning on STOR-i Blog</title>
    <link>/categories/statistical-learning/</link>
    <description>Recent content in Statistical Learning on STOR-i Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/statistical-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An Intro to Classification</title>
      <link>/blog/2018-04-22-an-intro-to-classification/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-22-an-intro-to-classification/</guid>
      <description>So, this is a follow on from the Supervised or not blog post where I looked at how to decide if a problem is supervised or unsupervised and looked at a simple example on the iris dataset. Similar to that post, here I’ll look at classification again, but we’ll go more in-depth into some issues with classification.
Linear Discriminant Analysis In the previous post we’ve used K-nn, here we’ll use Linear discriminant analysis (LDA) which is slightly more complicated.</description>
    </item>
    
    <item>
      <title>AI ain&#39;t here yet!</title>
      <link>/blog/2018-04-19-ai-aint-here-yet/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-19-ai-aint-here-yet/</guid>
      <description>This blog post is about a talk given by Prof. Michael Jordan at SysML conference on the 15th of February.. He&amp;rsquo;s a professor of Statistics in the Department of Electrical Engineering and Computer Science and the Department of Statistics at the University of California, Berkeley. He&amp;rsquo;s extremely well-known, and has over 130,000 citations on google scholar. This is very much a follow on post from my previous blog post The Two Cultures of Data Analysis.</description>
    </item>
    
    <item>
      <title>The Two Cultures of Data Analysis</title>
      <link>/blog/2018-03-26-the-two-cultures-of-data-analysis/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-26-the-two-cultures-of-data-analysis/</guid>
      <description>Much of the reading I do tends to end up leading me to many papers which seem to be carried out in the machine learning field rather than statistics. I always ask myself what’s the difference. There’s so much blurring between the two areas like topic modelling which is based off of Bayesian statistics but still is worked on primarily by people in the machine learning community. Then, there’re areas which fall more in the statistics field like Expectation-Maximization; and the computer science field like neural networks.</description>
    </item>
    
    <item>
      <title>Supervised or not?</title>
      <link>/blog/2018-03-20-supervised-or-not/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-20-supervised-or-not/</guid>
      <description>MACHINE LEARNING!!! So, if you’ve not heard of machine learning yet, you probably haven’t been watching any TV the last decade. Problem is machine learning is absolutely massive field.
This is the intro to a series of blog posts I plan on doing on various areas in machine learning, more specifically statistically backed methods therefore I call it statistical learning. In this blog post I aim to break down the two main areas that are generally focused on.</description>
    </item>
    
  </channel>
</rss>